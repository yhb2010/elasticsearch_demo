使用批量请求并调整其大小：
	显而易见的，优化性能应该使用批量请求。 批量的大小则取决于你的数据、分析和集群配置，不过每次批量数据 5–15 MB 大是个不错的起始点。注意这里说的是
	物理字节数大小。
段和合并：
	段合并的计算量庞大， 而且还要吃掉大量磁盘 I/O。
	默认值是 20 MB/s，对机械磁盘应该是个不错的设置。如果你用的是 SSD，可以考虑提高到 100–200 MB/s。测试验证对你的系统哪个值合适：
	PUT /_cluster/settings
	{
	    "persistent" : {
	        "indices.store.throttle.max_bytes_per_sec" : "100mb"
	    }
	}
	如果你在做批量导入，完全不在意搜索，你可以彻底关掉合并限流。这样让你的索引速度跑到你磁盘允许的极限：
	PUT /_cluster/settings
	{
	    "transient" : {
	        "indices.store.throttle.type" : "none"
	    }
	}
	设置限流类型为 none 彻底关闭合并限流。等你完成了导入，记得改回 merge 重新打开限流。
	如果你使用的是机械磁盘而非 SSD，你需要添加下面这个配置到你的 elasticsearch.yml 里：
	index.merge.scheduler.max_thread_count: 1
	机械磁盘在并发I/O支持方面比较差，所以我们需要降低每个索引并发访问磁盘的线程数。这个设置允许 max_thread_count + 2 个线程同时进行磁盘操作，也
	就是设置为 1 允许三个线程。

其他：
	如果你的搜索结果不需要近实时的准确度，考虑把每个索引的 index.refresh_interval 改到 30s 。如果你是在做大批量导入，导入期间你可以通过设置这个
	值为 -1 关掉刷新。别忘记在完工的时候重新开启它。
	如果你在做大批量导入，考虑通过设置 index.number_of_replicas: 0关闭副本。文档在复制的时候，整个文档内容都被发往副本节点，然后逐字的把索引过
	程重复一遍。这意味着每个副本也会执行分析、索引以及可能的合并过程。
	相反，如果你的索引是零副本，然后在写入完成后再开启副本，恢复过程本质上只是一个字节到字节的网络传输。相比重复索引过程，这个算是相当高效的了。
	如果你没有给每个文档自带 ID，使用 Elasticsearch 的自动 ID 功能。 这个为避免版本查找做了优化，因为自动生成的 ID 是唯一的。
	如果你在使用自己的 ID，尝试使用一种 Lucene 友好的 ID。包括零填充序列 ID、UUID-1 和纳秒；这些 ID 都是有一致的，压缩良好的序列模式。相反的，像
	UUID-4 这样的 ID，本质上是随机的，压缩比很低，会明显拖慢 Lucene。